{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0dc7652-b8ef-48e8-9939-1005a02e5429",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "# __From raw twitter scrape json to clean fine tune babyfood jsonl__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde83860-6f3b-40cd-abaf-ca0b1e92886a",
   "metadata": {},
   "source": [
    "## __snatch tweets from json__\n",
    "reads a file that is a list of dictionaries where the target (tweet) is the value with a key named \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7e0d106-6b79-41a2-8110-a244f583fcb0",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3363/1797441357.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Opening JSON file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'poetry_found.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mjson_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtext_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mkwarg\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0motherwise\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mJSONDecoder\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[0;32m--> 293\u001b[0;31m     return loads(fp.read(),\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 357\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.8.12/lib/python3.8/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "import json\n",
    " \n",
    "# Opening JSON file\n",
    "f = open('poetry_found.csv')\n",
    "json_file = json.load(f)\n",
    "\n",
    "text_file = []\n",
    "\n",
    "for dictionary in json_file:\n",
    "    text_file.append(dictionary[\"text\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8857b158-332c-40ab-a3ca-1cc8332cf8ce",
   "metadata": {},
   "source": [
    "## __tweet cleaner__\n",
    "if we decide hashtags, emojis or @mentions ae a part of contemporary expressions that belong into poetry we could comment out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d136b374-37d1-495a-921e-c33af216a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "\n",
    "\n",
    "def tweet_cleaner(tweet):\n",
    "    # remove mentions\n",
    "    mentions_free = re.sub(\"@\\w+\", \"\", tweet)\n",
    "    # remove hashtags\n",
    "    hashtag_free = re.sub(\"#\\w+\", \"\", mentions_free)\n",
    "    # special shortcuts\n",
    "    rt_free = re.sub(\"^RT\", \"\", hashtag_free)\n",
    "    # remove url \n",
    "    url_free = re.sub(r'http\\S+', '', rt_free)\n",
    "    # remove emojis\n",
    "    emo_free = emoji.get_emoji_regexp().sub(u'', url_free)\n",
    "    # remove white space\n",
    "    stripped = emo_free.strip()\n",
    "  \n",
    "    return stripped\n",
    "\n",
    "clean_tweets = [tweet_cleaner(x)for x in text_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ce1dc3-13c7-4b9c-9187-c4d6a28a04aa",
   "metadata": {},
   "source": [
    "## __unicode__\n",
    "this removes stuff like \\xa \\n and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7222510f-4793-4118-a7ac-ec4ec3cffbf9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def unicode_maker(text):\n",
    "    # creating a unicode string\n",
    "    text_encode = text.encode(encoding=\"ascii\", errors=\"ignore\")\n",
    "    text_decode = text_encode.decode()\n",
    "    clean_text = \" \".join([word for word in text_decode.split()])\n",
    "    return clean_text\n",
    "\n",
    "uni_tweets = [unicode_maker(x)for x in clean_tweets]\n",
    "uni_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00203a0-93c8-4771-af69-b315ae7d042e",
   "metadata": {},
   "source": [
    "## __remove punctuation, numbers etc.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48c5f047-61ad-4910-a225-d673e4cc061a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def number_punct(sentence):\n",
    "    # remove numbers\n",
    "    number_free = ''.join(word for word in sentence if not word.isdigit())\n",
    "    # remove punctuation\n",
    "    punctuation_free = \"\".join(\n",
    "        [i for i in number_free if i not in string.punctuation])\n",
    "    # lower case\n",
    "    upper_free = punctuation_free.lower()\n",
    "    \n",
    "    return upper_free\n",
    "\n",
    "number_free = [number_punct(x) for x in uni_tweets]\n",
    "number_free"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4101af-f17a-4e1b-8dc0-fadac82acb56",
   "metadata": {
    "tags": []
   },
   "source": [
    "## __remove short tweets, duplicates, empty strings, excess whitespace__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "66a6b090-d7f1-473e-a445-5849b9b02433",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## removes tweets with less than two words\n",
    "more_than_two = [x for x in number_free if (len(x.split())>2)]\n",
    "\n",
    "## removes duplicates\n",
    "clean_set = set(more_than_two)\n",
    "clean_set\n",
    "\n",
    "# remove empty strings\n",
    "no_empty_strings = [i for i in clean_set if i]\n",
    "# remove outer whitespace \n",
    "stripped = [x.strip() for x in no_empty_strings]\n",
    "# remove inner whitespace \n",
    "one_space =  [re.sub(' +', ' ', x) for x in stripped]\n",
    "one_space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d901eecc-a10f-4e9c-ab35-e1a342b451a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## __add whitespace in the beginning for openai's tokenization and a stop sign like END__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "63dd3ce9-abaa-44ea-9dda-efeea69ecfc0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_end = [f\" {x}. END\" for x in stripped] \n",
    "start_end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6194b3-44e9-4054-a323-c74b406e08a1",
   "metadata": {},
   "source": [
    "## __Creates a DataFrame with a prompt and completion column__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3fbef01e-abf5-46ef-8601-f99f3d45a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# keyword = json_file[0][\"segment\"]\n",
    "df_prompt_comp = pd.DataFrame({\"prompt\": \"\", \"completion\":start_end})\n",
    "df_dub_free = df_prompt_comp.drop_duplicates()\n",
    "\n",
    "#saves json babyfood\n",
    "df_dub_free.to_json(\"....json\", orient = \"records\", lines= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e486d657-0c26-421f-a1a0-51adc5fef5a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# __from txt file to babyfood__\n",
    "saves some song examples in a txt file and get json with linebreaks and end bit (END) starting whitespace - songparts are devided by blank line (\\n\\n respectively) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "237e526a-5a15-4848-8536-59c485af791a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46197e66-449c-4100-9cc7-64015b1ae2bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load txt file with song examples - examples are divided by whiteline\n",
    "rap_file = list(open(\"raps.txt\", \"r\"))\n",
    "rap_list = \"\".join(x for x in rap_file)\n",
    "\n",
    "# split examples by whiteline \\n\\n\n",
    "chunks = re.split('\\n\\n', rap_list)\n",
    "\n",
    "# replace \\' by '\n",
    "apo = [sub.replace(\"\\'\", \"'\") for sub in chunks]\n",
    "\n",
    "# remove empty strings\n",
    "no_empty_rap = [i for i in apo if i]\n",
    "\n",
    "# put leading whitespace and END bit for better tokenization and GTP3 learning\n",
    "space_rap = [f\" {x} END\" for x in no_empty_rap] \n",
    "\n",
    "# remove unicode\n",
    "string_decode =  [x.encode(\"ascii\", \"ignore\").decode() for x in space_rap] \n",
    "\n",
    "# make a pandas Dataframe\n",
    "df_rap = pd.DataFrame({\"prompt\":\"\", \"completion\":string_decode})\n",
    "\n",
    "# save into a GTP3 learnable jsonl with linebreaks and END bit to give as a example document \n",
    "df_rap.to_json(\"raps.json\", orient = \"records\", lines= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4656e-653b-4da9-bfe9-440951875037",
   "metadata": {},
   "source": [
    "# __make csv with poems to babyfood__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6e39eb8a-224f-4cda-899f-4579a9eb20a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f828d8c-5ce9-486c-b9c0-b9885bde15be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_poetry = pd.read_csv(\"poetry_found.csv\")\n",
    "df_poetry2 = df_poetry[\"Poem\"]\n",
    "\n",
    "no_rr =  [re.sub(r'\\r\\r', '', x) for x in df_poetry2]\n",
    "no_nnnn =  [re.sub(r'\\n\\n\\n\\n', '\\n', x) for x in no_rr]\n",
    "no_nnn =  [re.sub(r'\\n\\n\\n', '\\n', x) for x in no_nnnn]\n",
    "no_nn =  [re.sub(r'\\n\\n', '\\n', x) for x in no_nnn]\n",
    "no_n =  [re.sub(r'\\n \\n', '\\n', x) for x in no_nn]\n",
    "stripped = [x.strip(\"\\n\") for x in no_n]\n",
    "decode =  [x.encode(\"ascii\", \"ignore\").decode() for x in stripped]\n",
    "not_empty = [i for i in decode if i]\n",
    "poem_onespace =  [re.sub(' +', ' ', x) for x in not_empty]\n",
    "\n",
    "end_poem = [f\" {x} END\" for x in poem_onespace] \n",
    "df_poem = pd.DataFrame({\"prompt\":\"\", \"completion\":end_poem})\n",
    "df_poem_singular = df_poem.drop_duplicates()\n",
    "\n",
    "df_train, df_test = train_test_split(df_poem_singular, test_size=0.41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "de0aa18d-752e-4051-b550-0587920d0396",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.to_json(\"poem_train.json\", orient = \"records\", lines= True)\n",
    "df_test.to_json(\"poem_test.json\", orient = \"records\", lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8f97a3a5-5328-4f41-821d-0b22fcd3d0ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13747, 2)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_poem_singular.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "97e2b6bb-4c97-4a3d-9262-d1c31e33ae0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8110, 2)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ffc63046-a047-4cb8-8a2f-c5b31cccc37b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5637, 2)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
